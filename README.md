# ML Model Comparison Dashboard

An end-to-end Machine Learning project that compares multiple classification models using a reusable evaluation pipeline and an interactive Streamlit dashboard.

This project demonstrates real-world ML workflow skills such as:
- Training multiple models consistently
- Evaluating and comparing performance metrics
- Building internal ML dashboards
- Reproducible experimentation

---

##  Features
- Train multiple ML models with a single evaluation utility
- Metrics: Accuracy, Precision, Recall, F1-score
- Auto-generated `metrics.json` for tracking results
- Interactive Streamlit dashboard:
  - Model selection dropdown
  - Metrics table
  - Model comparison chart
  - Retrain models button

=======

---

## Why this project matters (Real-world relevance)

In real companies, ML engineers:
- test multiple models before choosing one
- track performance metrics consistently
- retrain models when data changes
- communicate results using dashboards

This project simulates that exact workflow.

---



##  Tech Stack
- Python
- pandas
- scikit-learn
- Streamlit
- matplotlib
- joblib


---
=======